{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install moabb","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:32:03.458855Z","iopub.execute_input":"2024-04-20T05:32:03.459338Z","iopub.status.idle":"2024-04-20T05:32:54.278838Z","shell.execute_reply.started":"2024-04-20T05:32:03.459301Z","shell.execute_reply":"2024-04-20T05:32:54.277253Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting moabb\n  Downloading moabb-1.0.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: PyYAML<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from moabb) (6.0.1)\nCollecting coverage<8.0.0,>=7.0.1 (from moabb)\n  Downloading coverage-7.4.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\nCollecting edflib-python<2.0.0,>=1.0.6 (from moabb)\n  Downloading EDFlib_Python-1.0.8-py3-none-any.whl.metadata (1.3 kB)\nCollecting h5py<=3.8.0 (from moabb)\n  Downloading h5py-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\nRequirement already satisfied: matplotlib<4.0.0,>=3.6.2 in /opt/conda/lib/python3.10/site-packages (from moabb) (3.7.5)\nRequirement already satisfied: memory-profiler<0.62.0,>=0.61.0 in /opt/conda/lib/python3.10/site-packages (from moabb) (0.61.0)\nRequirement already satisfied: mne<2.0,>=1.4 in /opt/conda/lib/python3.10/site-packages (from moabb) (1.6.1)\nCollecting mne-bids<0.14,>=0.13 (from moabb)\n  Downloading mne_bids-0.13-py2.py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: numpy<2.0,>=1.22 in /opt/conda/lib/python3.10/site-packages (from moabb) (1.26.4)\nCollecting pandas<2.0.0,>=1.5.2 (from moabb)\n  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: pooch<2.0.0,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from moabb) (1.8.1)\nCollecting pyriemann<0.6,>=0.5 (from moabb)\n  Downloading pyriemann-0.5.tar.gz (119 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting pytest<8.0.0,>=7.4.0 (from moabb)\n  Downloading pytest-7.4.4-py3-none-any.whl.metadata (7.9 kB)\nRequirement already satisfied: requests<3.0.0,>=2.28.1 in /opt/conda/lib/python3.10/site-packages (from moabb) (2.31.0)\nRequirement already satisfied: scikit-learn<2.0.0,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from moabb) (1.2.2)\nRequirement already satisfied: scipy<2.0.0,>=1.9.3 in /opt/conda/lib/python3.10/site-packages (from moabb) (1.11.4)\nRequirement already satisfied: seaborn<0.13.0,>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from moabb) (0.12.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from moabb) (4.66.1)\nRequirement already satisfied: urllib3<2.0.0,>=1.26.15 in /opt/conda/lib/python3.10/site-packages (from moabb) (1.26.18)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (2.9.0.post0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from memory-profiler<0.62.0,>=0.61.0->moabb) (5.9.3)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from mne<2.0,>=1.4->moabb) (5.1.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from mne<2.0,>=1.4->moabb) (3.1.2)\nRequirement already satisfied: lazy-loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from mne<2.0,>=1.4->moabb) (0.3)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0.0,>=1.5.2->moabb) (2023.3.post1)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch<2.0.0,>=1.6.0->moabb) (4.2.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from pyriemann<0.6,>=0.5->moabb) (1.4.0)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest<8.0.0,>=7.4.0->moabb) (2.0.0)\nRequirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from pytest<8.0.0,>=7.4.0->moabb) (1.4.0)\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest<8.0.0,>=7.4.0->moabb) (1.2.0)\nRequirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytest<8.0.0,>=7.4.0->moabb) (2.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.28.1->moabb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.28.1->moabb) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.28.1->moabb) (2024.2.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.2.0->moabb) (3.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.6.2->moabb) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->mne<2.0,>=1.4->moabb) (2.1.3)\nDownloading moabb-1.0.0-py3-none-any.whl (563 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.8/563.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coverage-7.4.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (233 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading EDFlib_Python-1.0.8-py3-none-any.whl (26 kB)\nDownloading h5py-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading mne_bids-0.13-py2.py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pytest-7.4.4-py3-none-any.whl (325 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.3/325.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyriemann\n  Building wheel for pyriemann (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyriemann: filename=pyriemann-0.5-py2.py3-none-any.whl size=107752 sha256=c83075909c01d60945a42cc92c47b38cea730f6546c0e8988d4612fec354fb9a\n  Stored in directory: /root/.cache/pip/wheels/84/86/79/622e9c1dc933dc088e287ebfaac5aa9bdc6a38a9db193ce1f1\nSuccessfully built pyriemann\nInstalling collected packages: h5py, edflib-python, coverage, pytest, pandas, pyriemann, mne-bids, moabb\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.10.0\n    Uninstalling h5py-3.10.0:\n      Successfully uninstalled h5py-3.10.0\n  Attempting uninstall: pytest\n    Found existing installation: pytest 8.1.1\n    Uninstalling pytest-8.1.1:\n      Successfully uninstalled pytest-8.1.1\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.6 which is incompatible.\ndask-expr 1.0.11 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\nfeaturetools 1.30.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmizani 0.11.1 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nplotnine 0.13.4 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nwoodwork 0.30.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nxarray 2024.3.0 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed coverage-7.4.4 edflib-python-1.0.8 h5py-3.8.0 mne-bids-0.13 moabb-1.0.0 pandas-1.5.3 pyriemann-0.5 pytest-7.4.4\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport moabb","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:32:54.28089Z","iopub.execute_input":"2024-04-20T05:32:54.281272Z","iopub.status.idle":"2024-04-20T05:33:11.921463Z","shell.execute_reply.started":"2024-04-20T05:32:54.281239Z","shell.execute_reply":"2024-04-20T05:33:11.920292Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_types is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_channels_regexp is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.channel_type is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n2024-04-20 05:33:00.340403: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-20 05:33:00.340558: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-20 05:33:00.522659: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"To use the get_shape_from_baseconcar, InputShapeSetterEEG, BraindecodeDatasetLoaderyou need to install `braindecode`.`pip install braindecode` or Please refer to `https://braindecode.org`.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/moabb/pipelines/__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n  warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"Base class for a dataset.\"\"\"\n\nimport abc\nimport logging\nimport re\nimport traceback\nfrom dataclasses import dataclass\nfrom inspect import signature\nfrom pathlib import Path\nfrom typing import Dict, Union\n\nfrom sklearn.pipeline import Pipeline\n\nfrom moabb.datasets.bids_interface import StepType, _interface_map\nfrom moabb.datasets.preprocessing import SetRawAnnotations\n\n\nlog = logging.getLogger(__name__)\n\n\n@dataclass\nclass CacheConfig:\n    \"\"\"\n    Configuration for caching of datasets.\n\n    Parameters\n    ----------\n    save_*: bool\n        This flag specifies whether to save the output of the corresponding\n        step to disk.\n    use: bool\n        This flag specifies whether to use the disk cache in case it exists.\n        If True, the Raw or Epochs objects returned will not be preloaded\n        (this saves some time). Otherwise, they will be preloaded.\n        If use is False, the save_* and overwrite_* keys will be ignored.\n    overwrite_*: bool\n        This flag specifies whether to overwrite the disk cache in\n        case it exist.\n    path : None | str\n        Location of where to look for the data storing location.\n        If None, the environment variable or config parameter\n        ``MNE_DATASETS_(signifier)_PATH`` is used. If it doesn't exist, the\n        \"~/mne_data\" directory is used. If the dataset\n        is not found under the given path, the data\n        will be automatically downloaded to the specified folder.\n    verbose:\n        Verbosity level. See mne.verbose.\n\n    Notes\n    -----\n\n    .. versionadded:: 1.0.0\n\n    \"\"\"\n\n    save_raw: bool = False\n    save_epochs: bool = False\n    save_array: bool = False\n\n    use: bool = False\n\n    overwrite_raw: bool = False\n    overwrite_epochs: bool = False\n    overwrite_array: bool = False\n\n    path: Union[str, Path] = None\n    verbose: str = None\n\n    @classmethod\n    def make(cls, dic: Union[None, Dict, \"CacheConfig\"] = None) -> \"CacheConfig\":\n        \"\"\"\n        Create a CacheConfig object from a dict or another CacheConfig object.\n\n        Examples\n        -------\n        Using default parameters:\n\n        >>> CacheConfig.make()\n        CacheConfig(save=True, use=True, overwrite=True, path=None)\n\n        From a dict:\n\n        >>> dic = {'save': False}\n        >>> CacheConfig.make(dic)\n        CacheConfig(save=False, use=True, overwrite=True, path=None)\n        \"\"\"\n        if dic is None:\n            return cls()\n        elif isinstance(dic, dict):\n            return cls(**dic)\n        elif isinstance(dic, cls):\n            return dic\n        else:\n            raise ValueError(f\"Expected dict or CacheConfig, got {type(dic)}\")\n\n\ndef apply_step(pipeline, obj):\n    \"\"\"Apply a pipeline to an object.\"\"\"\n    if obj is None:\n        return None\n    try:\n        return pipeline.transform(obj)\n    except ValueError as error:\n        # no events received by RawToEpochs:\n        if str(error) == \"No events found\":\n            return None\n        raise error\n\n\ndef is_camel_kebab_case(name: str):\n    \"\"\"Check if a string is in CamelCase but can also contain dashes.\"\"\"\n    return re.fullmatch(r\"[a-zA-Z0-9\\-]+\", name) is not None\n\n\ndef is_abbrev(abbrev_name: str, full_name: str):\n    \"\"\"Check if abbrev_name is an abbreviation of full_name,\n    i.e. ifthe characters in abbrev_name are all in full_name\n    and in the same order. They must share the same capital letters.\"\"\"\n    pattern = re.sub(r\"([A-Za-z])\", r\"\\1[a-z0-9\\-]*\", re.escape(abbrev_name))\n    return re.fullmatch(pattern, full_name) is not None\n\n\ndef check_subject_names(data):\n    for subject in data.keys():\n        if not isinstance(subject, int):\n            raise ValueError(\n                f\"Subject names must be integers, found {type(subject)}: {subject!r}. \"\n                f\"If you used cache, you may need to erase it using overwrite=True.\"\n            )\n\n\ndef session_run_pattern():\n    return r\"([0-9]+)(|[a-zA-Z]+[a-zA-Z0-9]*)\"  # g1: index, g2: description\n\n\nconstraint_message = (\n    \"names must be strings starting with an integer \"\n    \"identifying the order in which they were recorded, \"\n    \"optionally followed by a description only containing \"\n    \"letters and numbers.\"\n)\n\n\ndef check_session_names(data):\n    pattern = session_run_pattern()\n    for subject, sessions in data.items():\n        indexes = []\n        for session in sessions.keys():\n            match = re.fullmatch(pattern, session)\n            if not isinstance(session, str) or not match:\n                raise ValueError(\n                    f\"Session {constraint_message} Found key {session!r} instead. \"\n                    f\"If you used cache, you may need to erase it using overwrite=True.\"\n                )\n            indexes.append(int(match.groups()[0]))\n        if not len(indexes) == len(set(indexes)):\n            raise ValueError(\n                f\"Session {constraint_message} Found duplicate index {list(sessions.keys())}.\"\n            )\n\n\ndef check_run_names(data):\n    pattern = session_run_pattern()\n    for subject, sessions in data.items():\n        for session, runs in sessions.items():\n            indexes = []\n            for run in runs.keys():\n                match = re.fullmatch(pattern, run)\n                if not isinstance(run, str) or not match:\n                    raise ValueError(\n                        f\"Run {constraint_message} Found key {run!r} instead. \"\n                        f\"If you used cache, you may need to erase it using overwrite=True.\"\n                    )\n                indexes.append(int(match.groups()[0]))\n            if not len(indexes) == len(set(indexes)):\n                raise ValueError(\n                    f\"Run {constraint_message} Found duplicate index {list(runs.keys())}.\"\n                )\n\n\nclass BaseDataset(metaclass=abc.ABCMeta):\n    \"\"\"Abstract Moabb BaseDataset.\n\n    Parameters required for all datasets\n\n    parameters\n    ----------\n    subjects: List of int\n        List of subject number (or tuple or numpy array)\n\n    sessions_per_subject: int\n        Number of sessions per subject (if varying, take minimum)\n\n    events: dict of strings\n        String codes for events matched with labels in the stim channel.\n        Currently imagery codes codes can include:\n        - left_hand\n        - right_hand\n        - hands\n        - feet\n        - rest\n        - left_hand_right_foot\n        - right_hand_left_foot\n        - tongue\n        - navigation\n        - subtraction\n        - word_ass (for word association)\n\n    code: string\n        Unique identifier for dataset, used in all plots.\n        The code should be in CamelCase.\n\n    interval: list with 2 entries\n        Imagery interval as defined in the dataset description\n\n    paradigm: ['p300','imagery', 'ssvep']\n        Defines what sort of dataset this is\n\n    doi: DOI for dataset, optional (for now)\n    \"\"\"\n\n    def __init__(\n        self,\n        subjects,\n        sessions_per_subject,\n        events,\n        code,\n        interval,\n        paradigm,\n        doi=None,\n        unit_factor=1e6,\n    ):\n        \"\"\"Initialize function for the BaseDataset.\"\"\"\n        try:\n            _ = iter(subjects)\n        except TypeError:\n            raise ValueError(\"subjects must be a iterable, like a list\") from None\n\n        if not is_camel_kebab_case(code):\n            raise ValueError(\n                f\"code {code!r} must be in Camel-KebabCase; \"\n                \"i.e. use CamelCase, and add dashes where absolutely necessary. \"\n                \"See moabb.datasets.base.is_camel_kebab_case for more information.\"\n            )\n        class_name = self.__class__.__name__.replace(\"_\", \"-\")\n        if not is_abbrev(class_name, code):\n            log.warning(\n                f\"The dataset class name {class_name!r} must be an abbreviation \"\n                f\"of its code {code!r}. \"\n                \"See moabb.datasets.base.is_abbrev for more information.\"\n            )\n\n        self.subject_list = subjects\n        self.n_sessions = sessions_per_subject\n        self.event_id = events\n        self.code = code\n        self.interval = interval\n        self.paradigm = paradigm\n        self.doi = doi\n        self.unit_factor = unit_factor\n\n    def _create_process_pipeline(self):\n        return Pipeline(\n            [\n                (\n                    StepType.RAW,\n                    SetRawAnnotations(\n                        self.event_id,\n                        durations=self.interval[1] - self.interval[0],\n                    ),\n                ),\n            ]\n        )\n\n    def get_data(\n        self,\n        subjects=None,\n        cache_config=None,\n        process_pipeline=None,\n    ):\n        \"\"\"\n        Return the data corresponding to a list of subjects.\n\n        The returned data is a dictionary with the following structure::\n\n            data = {'subject_id' :\n                        {'session_id':\n                            {'run_id': run}\n                        }\n                    }\n\n        subjects are on top, then we have sessions, then runs.\n        A sessions is a recording done in a single day, without removing the\n        EEG cap. A session is constitued of at least one run. A run is a single\n        contiguous recording. Some dataset break session in multiple runs.\n\n        Processing steps can optionally be applied to the data using the\n        ``*_pipeline`` arguments. These pipelines are applied in the\n        following order: ``raw_pipeline`` -> ``epochs_pipeline`` ->\n        ``array_pipeline``. If a ``*_pipeline`` argument is ``None``,\n        the step will be skipped. Therefore, the ``array_pipeline`` may\n        either receive a :class:`mne.io.Raw` or a :class:`mne.Epochs` object\n        as input depending on whether ``epochs_pipeline`` is ``None`` or not.\n\n        Parameters\n        ----------\n        subjects: List of int\n            List of subject number\n        cache_config: dict | CacheConfig\n            Configuration for caching of datasets. See ``CacheConfig``\n            for details.\n        process_pipeline: Pipeline | None\n            Optional processing pipeline to apply to the data.\n            To generate an adequate pipeline, we recommend using\n            :func:`moabb.utils.make_process_pipelines`.\n            This pipeline will receive :class:`mne.io.BaseRaw` objects.\n            The steps names of this pipeline should be elements of :class:`StepType`.\n            According to their name, the steps should either return a\n            :class:`mne.io.BaseRaw`, a :class:`mne.Epochs`, or a :func:`numpy.ndarray`.\n            This pipeline must be \"fixed\" because it will not be trained,\n            i.e. no call to ``fit`` will be made.\n\n        Returns\n        -------\n        data: Dict\n            dict containing the raw data\n        \"\"\"\n        if subjects is None:\n            subjects = self.subject_list\n\n        if not isinstance(subjects, list):\n            raise ValueError(\"subjects must be a list\")\n\n        cache_config = CacheConfig.make(cache_config)\n\n        if process_pipeline is None:\n            process_pipeline = self._create_process_pipeline()\n\n        data = dict()\n        for subject in subjects:\n            if subject not in self.subject_list:\n                raise ValueError(\"Invalid subject {:d} given\".format(subject))\n            data[subject] = self._get_single_subject_data_using_cache(\n                subject,\n                cache_config,\n                process_pipeline,\n            )\n        check_subject_names(data)\n        check_session_names(data)\n        check_run_names(data)\n        return data\n\n    def download(\n        self,\n        subject_list=None,\n        path=None,\n        force_update=False,\n        update_path=None,\n        accept=False,\n        verbose=None,\n    ):\n        \"\"\"Download all data from the dataset.\n\n        This function is only useful to download all the dataset at once.\n\n\n        Parameters\n        ----------\n        subject_list : list of int | None\n            List of subjects id to download, if None all subjects\n            are downloaded.\n        path : None | str\n            Location of where to look for the data storing location.\n            If None, the environment variable or config parameter\n            ``MNE_DATASETS_(dataset)_PATH`` is used. If it doesn't exist, the\n            \"~/mne_data\" directory is used. If the dataset\n            is not found under the given path, the data\n            will be automatically downloaded to the specified folder.\n        force_update : bool\n            Force update of the dataset even if a local copy exists.\n        update_path : bool | None\n            If True, set the MNE_DATASETS_(dataset)_PATH in mne-python\n            config to the given path. If None, the user is prompted.\n        accept: bool\n            Accept licence term to download the data, if any. Default: False\n        verbose : bool, str, int, or None\n            If not None, override default verbose level\n            (see :func:`mne.verbose`).\n        \"\"\"\n        if subject_list is None:\n            subject_list = self.subject_list\n        for subject in subject_list:\n            # check if accept is needed\n            sig = signature(self.data_path)\n            if \"accept\" in [str(p) for p in sig.parameters]:\n                self.data_path(\n                    subject=subject,\n                    path=path,\n                    force_update=force_update,\n                    update_path=update_path,\n                    verbose=verbose,\n                    accept=accept,\n                )\n            else:\n                self.data_path(\n                    subject=subject,\n                    path=path,\n                    force_update=force_update,\n                    update_path=update_path,\n                    verbose=verbose,\n                )\n\n    def _get_single_subject_data_using_cache(\n        self, subject, cache_config, process_pipeline\n    ):\n        \"\"\"Load a single subject's data using cache.\n\n        Either load the data of a single subject from disk cache or from the\n        dataset object,\n        then eventually saves or overwrites the cache version depending on the\n        parameters.\n        \"\"\"\n        steps = list(process_pipeline.steps)\n        splitted_steps = []  # list of (cached_steps, remaining_steps)\n        if cache_config.use:\n            splitted_steps += [\n                (steps[:i], steps[i:]) for i in range(len(steps), 0, -1)\n            ]  # [len(steps)...1]\n        splitted_steps.append(\n            ([], steps)\n        )  # last option:  if cached_steps is [], we don't use cache, i.e. i=0\n\n        for cached_steps, remaining_steps in splitted_steps:\n            sessions_data = None\n            # Load and eventually overwrite:\n            if len(cached_steps) == 0:  # last option: we don't use cache\n                sessions_data = self._get_single_subject_data(subject)\n                assert sessions_data is not None  # should not happen\n            else:\n                cache_type = cached_steps[-1][0]\n                interface = _interface_map[cache_type](\n                    self,\n                    subject,\n                    path=cache_config.path,\n                    process_pipeline=Pipeline(cached_steps),\n                    verbose=cache_config.verbose,\n                )\n\n                if (\n                    (cache_config.overwrite_raw and cache_type is StepType.RAW)\n                    or (cache_config.overwrite_epochs and cache_type is StepType.EPOCHS)\n                    or (cache_config.overwrite_array and cache_type is StepType.ARRAY)\n                ):\n                    interface.erase()\n                elif cache_config.use:  # can't load if it was just erased\n                    sessions_data = interface.load(\n                        preload=False\n                    )  # None if cache inexistent\n\n            # If no cache was found or if it was erased, try the next option:\n            if sessions_data is None:\n                continue\n\n            # Apply remaining steps and save:\n            for step_idx, (step_type, process_pipeline) in enumerate(remaining_steps):\n                # apply one step:\n                sessions_data = {\n                    session: {\n                        run: apply_step(process_pipeline, raw)\n                        for run, raw in runs.items()\n                    }\n                    for session, runs in sessions_data.items()\n                }\n\n                # save:\n                if (\n                    (\n                        cache_config.save_raw\n                        and step_type is StepType.RAW\n                        and (\n                            (step_idx == len(remaining_steps) - 1)\n                            or (remaining_steps[step_idx + 1][0] is not StepType.RAW)\n                        )\n                    )  # we only save the last raw step\n                    or (cache_config.save_epochs and step_type is StepType.EPOCHS)\n                    or (cache_config.save_array and step_type is StepType.ARRAY)\n                ):\n                    interface = _interface_map[step_type](\n                        self,\n                        subject,\n                        path=cache_config.path,\n                        process_pipeline=Pipeline(\n                            cached_steps + remaining_steps[: step_idx + 1]\n                        ),\n                        verbose=cache_config.verbose,\n                    )\n                    try:\n                        interface.save(sessions_data)\n                    except Exception:\n                        log.warning(\n                            f\"Failed to save {interface.__repr__()} \"\n                            f\"to BIDS format:\\n\"\n                            f\"{' Pipeline: '.center(50, '#')}\\n\"\n                            f\"{interface.process_pipeline.__repr__()}\\n\"\n                            f\"{' Exception: '.center(50, '#')}\\n\"\n                            f\"{''.join(traceback.format_exc())}{'#' * 50}\"\n                        )\n                        interface.erase()  # remove partial cache\n            return sessions_data\n        raise ValueError(\"should not happen\")\n\n    @abc.abstractmethod\n    def _get_single_subject_data(self, subject):\n        \"\"\"Return the data of a single subject.\n\n        The returned data is a dictionary with the following structure\n\n        data = {'session_id':\n                    {'run_id': raw}\n                }\n\n        parameters\n        ----------\n        subject: int\n            subject number\n\n        returns\n        -------\n        data: Dict\n            dict containing the raw data\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def data_path(\n        self, subject, path=None, force_update=False, update_path=None, verbose=None\n    ):\n        \"\"\"Get path to local copy of a subject data.\n\n        Parameters\n        ----------\n        subject : int\n            Number of subject to use\n        path : None | str\n            Location of where to look for the data storing location.\n            If None, the environment variable or config parameter\n            ``MNE_DATASETS_(dataset)_PATH`` is used. If it doesn't exist, the\n            \"~/mne_data\" directory is used. If the dataset\n            is not found under the given path, the data\n            will be automatically downloaded to the specified folder.\n        force_update : bool\n            Force update of the dataset even if a local copy exists.\n        update_path : bool | None **Deprecated**\n            If True, set the MNE_DATASETS_(dataset)_PATH in mne-python\n            config to the given path. If None, the user is prompted.\n        verbose : bool, str, int, or None\n            If not None, override default verbose level\n            (see :func:`mne.verbose`).\n\n        Returns\n        -------\n        path : list of str\n            Local path to the given data file. This path is contained inside a\n            list of length one, for compatibility.\n        \"\"\"  # noqa: E501\n        pass\n\"\"\"\nSimple and compound motor imagery\nhttps://doi.org/10.1371/journal.pone.0114853\n\"\"\"\n# Author: Alexandre Barachant <alexandre.barachant@gmail.com>\n#         Sylvain Chevallier <sylvain.chevallier@uvsq.fr>\n#         Bruno Aristimunha <b.aristimunha@gmail.com>\n# License: BSD Style.\n\nimport json\nimport os\nimport os.path as osp\nfrom pathlib import Path\n\nimport requests\nfrom mne import get_config, set_config\nfrom mne.datasets.utils import _get_path\nfrom mne.utils import _url_to_local_path, verbose\nfrom pooch import file_hash, retrieve\nfrom pooch.downloaders import choose_downloader\nfrom requests.exceptions import HTTPError\n\n\ndef get_dataset_path(sign, path):\n    \"\"\"Returns the dataset path allowing for changes in MNE_DATA config.\n\n    Parameters\n    ----------\n    sign : str\n        Signifier of dataset\n    path : None | str\n        Location of where to look for the data storing location.\n        If None, the environment variable or config parameter\n        ``MNE_DATASETS_(signifier)_PATH`` is used. If it doesn't exist, the\n        \"~/mne_data\" directory is used. If the dataset\n        is not found under the given path, the data\n        will be automatically downloaded to the specified folder.\n\n    Returns\n    -------\n        path : None | str\n        Location of where to look for the data storing location\n    \"\"\"\n    sign = sign.upper()\n    key = \"MNE_DATASETS_{:s}_PATH\".format(sign)\n    if get_config(key) is None:\n        if get_config(\"MNE_DATA\") is None:\n            path_def = Path.home() / \"mne_data\"\n            print(\n                \"MNE_DATA is not already configured. It will be set to \"\n                \"default location in the home directory - \"\n                + str(path_def)\n                + \"\\nAll datasets will be downloaded to this location, if anything is \"\n                \"already downloaded, please move manually to this location\"\n            )\n            if not path_def.is_dir():\n                path_def.mkdir(parents=True)\n            set_config(\"MNE_DATA\", str(Path.home() / \"mne_data\"))\n        set_config(key, get_config(\"MNE_DATA\"))\n    return _get_path(path, key, sign)\n\n\n@verbose\ndef data_path(url, sign, path=None, force_update=False, update_path=True, verbose=None):\n    \"\"\"Get path to local copy of given dataset URL. **Deprecated**\n\n    This is a low-level function useful for getting a local copy of a\n    remote dataset. It is deprecated in favor of data_dl.\n\n    Parameters\n    ----------\n    url : str\n        Path to remote location of data\n    sign : str\n        Signifier of dataset\n    path : None | str\n        Location of where to look for the data storing location.\n        If None, the environment variable or config parameter\n        ``MNE_DATASETS_(signifier)_PATH`` is used. If it doesn't exist, the\n        \"~/mne_data\" directory is used. If the dataset\n        is not found under the given path, the data\n        will be automatically downloaded to the specified folder.\n    force_update : bool\n        Force update of the dataset even if a local copy exists.\n    update_path : bool | None, **Deprecated**\n        Unused, kept for compatibility purpose.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`).\n\n    Returns\n    -------\n    path : list of str\n        Local path to the given data file. This path is contained inside a list\n        of length one, for compatibility.\n    \"\"\"  # noqa: E501\n    path = get_dataset_path(sign, path)\n    key_dest = \"MNE-{:s}-data\".format(sign.lower())\n    destination = _url_to_local_path(url, osp.join(path, key_dest))\n    # Fetch the file\n    if not osp.isfile(destination) or force_update:\n        if osp.isfile(destination):\n            os.remove(destination)\n        if not osp.isdir(osp.dirname(destination)):\n            os.makedirs(osp.dirname(destination))\n        retrieve(url, None, path=destination)\n    return destination\n\n\n@verbose\ndef data_dl(url, sign, path=None, force_update=False, verbose=None):\n    \"\"\"Download file from url to specified path.\n\n    This function should replace data_path as the MNE will not support the download\n    of dataset anymore. This version is using Pooch.\n\n    Parameters\n    ----------\n    url : str\n        Path to remote location of data\n    sign : str\n        Signifier of dataset\n    path : None | str\n        Location of where to look for the data storing location.\n        If None, the environment variable or config parameter\n        ``MNE_DATASETS_(signifier)_PATH`` is used. If it doesn't exist, the\n        \"~/mne_data\" directory is used. If the dataset\n        is not found under the given path, the data\n        will be automatically downloaded to the specified folder.\n    force_update : bool\n        Force update of the dataset even if a local copy exists.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`).\n\n    Returns\n    -------\n    path : list of str\n        Local path to the given data file. This path is contained inside a list\n        of length one, for compatibility.\n    \"\"\"\n    path = Path(get_dataset_path(sign, path))\n    key_dest = \"MNE-{:s}-data\".format(sign.lower())\n    destination = _url_to_local_path(url, path / key_dest)\n    destination = str(path) + destination.split(str(path))[1]\n    table = {ord(c): \"-\" for c in ':*?\"<>|'}\n    destination = Path(str(path) + destination.split(str(path))[1].translate(table))\n\n    downloader = choose_downloader(url, progressbar=True)\n    if type(downloader).__name__ in [\"HTTPDownloader\", \"DOIDownloader\"]:\n        downloader.kwargs.setdefault(\"verify\", False)\n\n    # Fetch the file\n    if not destination.is_file() or force_update:\n        if destination.is_file():\n            destination.unlink()\n        destination.parent.mkdir(parents=True, exist_ok=True)\n        known_hash = None\n    else:\n        known_hash = file_hash(str(destination))\n    dlpath = retrieve(\n        url,\n        known_hash,\n        fname=Path(url).name,\n        path=str(destination.parent),\n        progressbar=True,\n        downloader=downloader,\n    )\n    return dlpath\n\n\n# This function is from https://github.com/cognoma/figshare (BSD-3-Clause)\ndef fs_issue_request(method, url, headers, data=None, binary=False):\n    \"\"\"Wrapper for HTTP request.\n\n    Parameters\n    ----------\n    method : str\n        HTTP method. One of GET, PUT, POST or DELETE\n    url : str\n        URL for the request\n    headers: dict\n        HTTP header information\n    data: dict\n        Figshare article data\n    binary: bool\n        Whether data is binary or not\n\n    Returns\n    -------\n    response_data: dict\n        JSON response for the request returned as python dict\n    \"\"\"\n    if data is not None and not binary:\n        data = json.dumps(data)\n\n    response = requests.request(method, url, headers=headers, data=data)\n\n    try:\n        response.raise_for_status()\n        try:\n            response_data = json.loads(response.text)\n        except ValueError:\n            response_data = response.content\n    except HTTPError as error:\n        print(\"Caught an HTTPError: {}\".format(error))\n        print(\"Body:\\n\", response.text)\n        raise\n\n    return response_data\n\n\ndef fs_get_file_list(article_id, version=None):\n    \"\"\"List all the files associated with a given article.\n\n    Parameters\n    ----------\n    article_id : str or int\n        Figshare article ID\n    version : str or id, default is None\n        Figshare article version. If None, selects the most recent version.\n\n    Returns\n    -------\n    response : dict\n        HTTP request response as a python dict\n    \"\"\"\n    fsurl = \"https://api.figshare.com/v2\"\n    if version is None:\n        url = fsurl + \"/articles/{}/files\".format(article_id)\n        headers = {\"Content-Type\": \"application/json\"}\n        response = fs_issue_request(\"GET\", url, headers=headers)\n        return response\n    else:\n        url = fsurl + \"/articles/{}/versions/{}\".format(article_id, version)\n        headers = {\"Content-Type\": \"application/json\"}\n        request = fs_issue_request(\"GET\", url, headers=headers)\n        return request[\"files\"]\n\n\ndef fs_get_file_hash(filelist):\n    \"\"\"Returns a dict associating figshare file id to MD5 hash.\n\n    Parameters\n    ----------\n    filelist : list of dict\n        HTTP request response from fs_get_file_list\n\n    Returns\n    -------\n    response : dict\n        keys are file_id and values are md5 hash\n    \"\"\"\n    return {str(f[\"id\"]): \"md5:\" + f[\"supplied_md5\"] for f in filelist}\n\n\ndef fs_get_file_id(filelist):\n    \"\"\"Returns a dict associating filename to figshare file id.\n\n    Parameters\n    ----------\n    filelist : list of dict\n        HTTP request response from fs_get_file_list\n\n    Returns\n    -------\n    response : dict\n        keys are filename and values are file_id\n    \"\"\"\n    return {f[\"name\"]: str(f[\"id\"]) for f in filelist}\n\n\ndef fs_get_file_name(filelist):\n    \"\"\"Returns a dict associating figshare file id to filename.\n\n    Parameters\n    ----------\n    filelist : list of dict\n        HTTP request response from fs_get_file_list\n\n    Returns\n    -------\n    response : dict\n        keys are file_id and values are file name\n    \"\"\"\n    return {str(f[\"id\"]): f[\"name\"] for f in filelist}\nimport logging\nimport os\nimport shutil\n\nimport mne\nimport numpy as np\nfrom pooch import Unzip, retrieve\nfrom scipy.io import loadmat\n\n# from .base import BaseDataset\n# from .download import get_dataset_path\n\n\nlog = logging.getLogger(__name__)\n\nFILES = []\nFILES.append(\"https://dataverse.harvard.edu/api/access/datafile/2499178\")\nFILES.append(\"https://dataverse.harvard.edu/api/access/datafile/2499182\")\nFILES.append(\"https://dataverse.harvard.edu/api/access/datafile/2499179\")\n\n\ndef eeg_data_path(base_path, subject):\n    file1_subj = [\"cl\", \"cyy\", \"kyf\", \"lnn\"]\n    file2_subj = [\"ls\", \"ry\", \"wcf\"]\n    file3_subj = [\"wx\", \"yyx\", \"zd\"]\n\n    def get_subjects(sub_inds, sub_names, ind):\n        dataname = \"data{}\".format(ind)\n        if not os.path.isfile(os.path.join(base_path, dataname + \".zip\")):\n            retrieve(\n                FILES[ind],\n                None,\n                dataname + \".zip\",\n                base_path,\n                processor=Unzip(),\n                progressbar=True,\n            )\n\n        for fname in os.listdir(os.path.join(base_path, dataname + \".zip.unzip\")):\n            for ind, prefix in zip(sub_inds, sub_names):\n                if fname.startswith(prefix):\n                    os.rename(\n                        os.path.join(base_path, dataname + \".zip.unzip\", fname),\n                        os.path.join(base_path, \"subject_{}.mat\".format(ind)),\n                    )\n        os.remove(os.path.join(base_path, dataname + \".zip\"))\n        shutil.rmtree(os.path.join(base_path, dataname + \".zip.unzip\"))\n\n    if not os.path.isfile(os.path.join(base_path, \"subject_{}.mat\".format(subject))):\n        if subject in range(1, 5):\n            get_subjects(list(range(1, 5)), file1_subj, 0)\n        elif subject in range(5, 8):\n            get_subjects(list(range(5, 8)), file2_subj, 1)\n        elif subject in range(8, 11):\n            get_subjects(list(range(8, 11)), file3_subj, 2)\n    return os.path.join(base_path, \"subject_{}.mat\".format(subject))\n\n\nclass Weibo2014(BaseDataset):\n    \"\"\"Motor Imagery dataset from Weibo et al 2014.\n\n    .. admonition:: Dataset summary\n\n\n        =========  =======  =======  ==========  =================  ============  ===============  ===========\n        Name         #Subj    #Chan    #Classes    #Trials / class  Trials len    Sampling rate      #Sessions\n        =========  =======  =======  ==========  =================  ============  ===============  ===========\n        Weibo2014       10       60           7                 80  4s            200Hz                      1\n        =========  =======  =======  ==========  =================  ============  ===============  ===========\n\n    Dataset from the article *Evaluation of EEG oscillatory patterns and\n    cognitive process during simple and compound limb motor imagery* [1]_.\n\n    It contains data recorded on 10 subjects, with 60 electrodes.\n\n    This dataset was used to investigate the differences of the EEG patterns\n    between simple limb motor imagery and compound limb motor\n    imagery. Seven kinds of mental tasks have been designed, involving three\n    tasks of simple limb motor imagery (left hand, right hand, feet), three\n    tasks of compound limb motor imagery combining hand with hand/foot\n    (both hands, left hand combined with right foot, right hand combined with\n    left foot) and rest state.\n\n    At the beginning of each trial (8 seconds), a white circle appeared at the\n    center of the monitor. After 2 seconds, a red circle (preparation cue)\n    appeared for 1 second to remind the subjects of paying attention to the\n    character indication next. Then red circle disappeared and character\n    indication (‘Left Hand’, ‘Left Hand & Right Foot’, et al) was presented on\n    the screen for 4 seconds, during which the participants were asked to\n    perform kinesthetic motor imagery rather than a visual type of imagery\n    while avoiding any muscle movement. After 7 seconds, ‘Rest’ was presented\n    for 1 second before next trial (Fig. 1(a)). The experiments were divided\n    into 9 sections, involving 8 sections consisting of 60 trials each for six\n    kinds of MI tasks (10 trials for each MI task in one section) and one\n    section consisting of 80 trials for rest state. The sequence of six MI\n    tasks was randomized. Intersection break was about 5 to 10 minutes.\n\n    References\n    -----------\n    .. [1] Yi, Weibo, et al. \"Evaluation of EEG oscillatory patterns and\n           cognitive process during simple and compound limb motor imagery.\"\n           PloS one 9.12 (2014). https://doi.org/10.1371/journal.pone.0114853\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            subjects=list(range(1, 11)),\n            sessions_per_subject=1,\n            events=dict(\n                left_hand=1,\n                right_hand=2,\n                hands=3,\n                feet=4,\n                left_hand_right_foot=5,\n                right_hand_left_foot=6,\n                rest=7,\n            ),\n            code=\"Weibo2014\",\n            # Full trial w/ rest is 0-8\n            interval=[3, 7],\n            paradigm=\"imagery\",\n            doi=\"10.1371/journal.pone.0114853\",\n        )\n\n    def _get_single_subject_data(self, subject):\n        \"\"\"Return data for a single subject.\"\"\"\n        fname = self.data_path(subject)\n        # TODO: add 1s 0 buffer between trials and make continuous\n        data = loadmat(\n            fname,\n            squeeze_me=True,\n            struct_as_record=False,\n            verify_compressed_data_integrity=False,\n        )\n        montage = mne.channels.make_standard_montage(\"standard_1005\")\n\n        # fmt: off\n        ch_names = [\n            \"Fp1\", \"Fpz\", \"Fp2\", \"AF3\", \"AF4\", \"F7\", \"F5\", \"F3\", \"F1\", \"Fz\", \"F2\", \"F4\", \"F6\",\n            \"F8\", \"FT7\", \"FC5\", \"FC3\", \"FC1\", \"FCz\", \"FC2\", \"FC4\", \"FC6\", \"FT8\", \"T7\", \"C5\",\n            \"C3\", \"C1\", \"Cz\", \"C2\", \"C4\", \"C6\", \"T8\", \"TP7\", \"CP5\", \"CP3\", \"CP1\", \"CPz\", \"CP2\",\n            \"CP4\", \"CP6\", \"TP8\", \"P7\", \"P5\", \"P3\", \"P1\", \"Pz\", \"P2\", \"P4\", \"P6\", \"P8\", \"PO7\",\n            \"PO5\", \"PO3\", \"POz\", \"PO4\", \"PO6\", \"PO8\", \"CB1\", \"O1\", \"Oz\", \"O2\", \"CB2\", \"VEO\", \"HEO\",\n        ]\n        # fmt: on\n\n        ch_types = [\"eeg\"] * 62 + [\"eog\"] * 2\n        # FIXME not sure what are those CB1 / CB2\n        ch_types[57] = \"misc\"\n        ch_types[61] = \"misc\"\n        info = mne.create_info(\n            ch_names=ch_names + [\"STIM014\"], ch_types=ch_types + [\"stim\"], sfreq=200\n        )\n        # until we get the channel names montage is None\n        event_ids = data[\"label\"].ravel()\n        raw_data = np.transpose(data[\"data\"], axes=[2, 0, 1])\n        # de-mean each trial\n        raw_data = raw_data - np.mean(raw_data, axis=2, keepdims=True)\n        raw_events = np.zeros((raw_data.shape[0], 1, raw_data.shape[2]))\n        raw_events[:, 0, 0] = event_ids\n        data = np.concatenate([1e-6 * raw_data, raw_events], axis=1)\n        # add buffer in between trials\n        log.warning(\n            \"Trial data de-meaned and concatenated with a buffer to create \" \"cont data\"\n        )\n        zeroshape = (data.shape[0], data.shape[1], 50)\n        data = np.concatenate([np.zeros(zeroshape), data, np.zeros(zeroshape)], axis=2)\n        raw = mne.io.RawArray(\n            data=np.concatenate(list(data), axis=1), info=info, verbose=False\n        )\n        raw.set_montage(montage)\n        return {\"0\": {\"0\": raw}}\n\n    def data_path(\n        self, subject, path=None, force_update=False, update_path=None, verbose=None\n    ):\n        if subject not in self.subject_list:\n            raise (ValueError(\"Invalid subject number\"))\n        path = get_dataset_path(\"WEIBO\", path)\n        basepath = os.path.join(path, \"MNE-weibo-2014\")\n        if not os.path.isdir(basepath):\n            os.makedirs(basepath)\n        return eeg_data_path(basepath, subject)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:33:11.923685Z","iopub.execute_input":"2024-04-20T05:33:11.924377Z","iopub.status.idle":"2024-04-20T05:33:12.054531Z","shell.execute_reply.started":"2024-04-20T05:33:11.924345Z","shell.execute_reply":"2024-04-20T05:33:12.053097Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset=Weibo2014()\ndata=dataset.get_data()\n# mne_chlis=data[1]['0']['0'].ch_names","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:33:12.057498Z","iopub.execute_input":"2024-04-20T05:33:12.058019Z","iopub.status.idle":"2024-04-20T05:36:30.327882Z","shell.execute_reply.started":"2024-04-20T05:33:12.057974Z","shell.execute_reply":"2024-04-20T05:36:30.326357Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"MNE_DATA is not already configured. It will be set to default location in the home directory - /root/mne_data\nAll datasets will be downloaded to this location, if anything is already downloaded, please move manually to this location\nAttempting to create new mne-python configuration file:\n/root/.mne/mne-python.json\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/2308438627.py:625: RuntimeWarning: Setting non-standard config type: \"MNE_DATASETS_WEIBO_PATH\"\n  set_config(key, get_config(\"MNE_DATA\"))\nDownloading data from 'https://dataverse.harvard.edu/api/access/datafile/2499178' to file '/root/mne_data/MNE-weibo-2014/data0.zip'.\n100%|█████████████████████████████████████| 1.76G/1.76G [00:00<00:00, 1.45TB/s]\nSHA256 hash of downloaded file: 8fd54c02ef5e9426a819b835624591bf01fa38c33c7dddad5096c02e2c80c8c0\nUse this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\nUnzipping contents of '/root/mne_data/MNE-weibo-2014/data0.zip' to '/root/mne_data/MNE-weibo-2014/data0.zip.unzip'\nDownloading data from 'https://dataverse.harvard.edu/api/access/datafile/2499182' to file '/root/mne_data/MNE-weibo-2014/data1.zip'.\n100%|██████████████████████████████████████| 1.27G/1.27G [00:00<00:00, 708GB/s]\nSHA256 hash of downloaded file: 3253aa175ac1d649dc22a303bed345b2f71c6c07eb5d9b2dca5ab8e4e2248557\nUse this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\nUnzipping contents of '/root/mne_data/MNE-weibo-2014/data1.zip' to '/root/mne_data/MNE-weibo-2014/data1.zip.unzip'\nDownloading data from 'https://dataverse.harvard.edu/api/access/datafile/2499179' to file '/root/mne_data/MNE-weibo-2014/data2.zip'.\n100%|██████████████████████████████████████| 1.32G/1.32G [00:00<00:00, 961GB/s]\nSHA256 hash of downloaded file: d2712a669cefc9e0593c390d9e2767554b91453727c7e5e2824d3c39e3f91288\nUse this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\nUnzipping contents of '/root/mne_data/MNE-weibo-2014/data2.zip' to '/root/mne_data/MNE-weibo-2014/data2.zip.unzip'\n","output_type":"stream"}]},{"cell_type":"code","source":"# import os\n# import numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrontal_channels = ['Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6', 'AFF1h', 'AFF2h', 'AFF5h', 'AFF6h', 'FFC1h', 'FFC2h', 'FFC3h', 'FFC4h', 'FFC5h', 'FFC6h', 'F9', 'F10', 'FT9', 'FT10', 'FpZ', 'AF7', 'AF3', 'AF4', 'AF8', 'F5', 'F1', 'F2', 'F6']\ncentral_channels = ['T7', 'C3', 'Cz', 'C4', 'T8', 'CP5', 'CP1', 'CP2', 'CP6', 'CCP1h', 'CCP2h', 'CCP3h', 'CCP4h', 'CCP5h', 'CCP6h', 'C5', 'C1', 'C2', 'C6']\nparietal_channels = ['P7', 'P3', 'Pz', 'P4', 'P8', 'PO3', 'PO4', 'PO7', 'PO8', 'POz', 'CPP1h', 'CPP2h', 'CPP3h', 'CPP4h', 'CPP5h', 'CPP6h', 'P5', 'P1', 'P2', 'P6']\noccipital_channels = ['O1', 'Oz', 'O2', 'OI1h', 'OI2h', 'POO1', 'POO2', 'POO9h', 'POO10h', 'POO5h', 'POO6h', 'POO7h', 'POO8h', 'POO3h', 'POO4h']\ntemporal_channels = ['T7', 'C3', 'Cz', 'C4', 'T8', 'TP9', 'TP10', 'TP7', 'TP8', 'P7', 'P8', 'TTP7h', 'TTP8h', 'TTP9h', 'TTP10h']\nk=1\nfor subject_id, subject_data in data.items():\n    for session_id, session_data in subject_data.items():\n        for run_id, raw in session_data.items():\n#             if(k==1 or k==2):\n#                 k+=1\n#                 continue\n            mne_chlis=raw.ch_names      \n#             print(mne_chlis)\n            occipital_channels1 = [channel for channel in mne_chlis if channel in occipital_channels]\n            frontal_channels1 = [channel for channel in mne_chlis if channel in frontal_channels]\n            central_channels1 = [channel for channel in mne_chlis if channel in central_channels]\n            parietal_channels1 = [channel for channel in mne_chlis if channel in parietal_channels]\n            temporal_channels1 = [channel for channel in mne_chlis if channel in temporal_channels]\n\n\n            regions = {\n              'occipital': {'channels': occipital_channels1, 'desired_num': 16},\n              'frontal': {'channels': frontal_channels1, 'desired_num': 24},\n              'central': {'channels': central_channels1, 'desired_num': 32},\n              'parietal': {'channels': parietal_channels1, 'desired_num': 32},\n              'temporal': {'channels': temporal_channels1, 'desired_num': 24},\n            }\n\n            new_data = []\n            something_empty=False\n            for region, info in regions.items():\n                raw1=raw.copy()\n                print(raw.ch_names, \"original_channel\")\n#                 print(type(raw1))\n                print(region)\n#                 print(info['channels'])\n#                 print(raw1.ch_names,\"avail channels\")\n                if(info['channels']==[]):\n                    something_empty=True\n                    break\n                raw1.pick(info['channels'])\n                df=raw1.get_data()\n    \n                if df.shape[0] > info['desired_num']:\n                    df = df[:info['desired_num']]\n                else:\n                    repeats = info['desired_num'] // df.shape[0]\n                    remainder = info['desired_num'] % df.shape[0]\n                    data_repeated = np.repeat(df, repeats, axis=0)\n                    data_remainder = df[:remainder]\n                    df = np.concatenate((data_repeated, data_remainder), axis=0)\n                new_data.append(df)\n\n            new_data = np.concatenate(new_data, axis=0)\n            num_samples = new_data.shape[1]\n\n            # Calculate the number of samples to be removed to make it divisible by 440\n            remainder = num_samples % 440\n\n            # Remove the last part of the new_data array\n            if remainder != 0:\n                new_data = new_data[:, :-remainder]\n\n            print(new_data.shape)\n            if(something_empty==False):\n            # Calculate the number of chunks\n                num_chunks = new_data.shape[1] // 440\n\n                # Split the data into chunks\n                chunks = np.array_split(new_data, num_chunks, axis=1)\n#                 chunks=np.array(chunks)\n                # Create a directory to store the .fif files\n                os.makedirs('root_folder', exist_ok=True)\n\n                # Save each chunk as a .fif file\n                for i, chunk in enumerate(chunks):\n                    if(i%1000==0):\n                        print(i)\n                    import contextlib\n                    with open(os.devnull, 'w') as devnull:\n                        with contextlib.redirect_stdout(devnull):\n                            chunk_info = mne.create_info(ch_names=128, sfreq=raw.info['sfreq'])\n                            chunk_raw = mne.io.RawArray(chunk, chunk_info)\n                            x=chunk_raw.get_data()\n                            print(x.shape)\n                            chunk_raw.save(os.path.join('root_folder', f'chunk_{i}_{subject_id}_{session_id}_{run_id}.fif'), overwrite=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:53:31.528291Z","iopub.execute_input":"2024-04-20T05:53:31.528781Z","iopub.status.idle":"2024-04-20T05:53:41.823798Z","shell.execute_reply.started":"2024-04-20T05:53:31.528731Z","shell.execute_reply":"2024-04-20T05:53:41.82181Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"['Fp1', 'Fpz', 'Fp2', 'AF3', 'AF4', 'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT7', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'FT8', 'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP7', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'TP8', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO5', 'PO3', 'POz', 'PO4', 'PO6', 'PO8', 'CB1', 'O1', 'Oz', 'O2', 'CB2', 'VEO', 'HEO', 'STIM014']\n['Fp1', 'Fpz', 'Fp2', 'AF3', 'AF4', 'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT7', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'FT8', 'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP7', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'TP8', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO5', 'PO3', 'POz', 'PO4', 'PO6', 'PO8', 'CB1', 'O1', 'Oz', 'O2', 'CB2', 'VEO', 'HEO', 'STIM014'] original_channel\noccipital\n['Fp1', 'Fpz', 'Fp2', 'AF3', 'AF4', 'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT7', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'FT8', 'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP7', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'TP8', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO5', 'PO3', 'POz', 'PO4', 'PO6', 'PO8', 'CB1', 'O1', 'Oz', 'O2', 'CB2', 'VEO', 'HEO', 'STIM014'] original_channel\nfrontal\n['Fp1', 'Fpz', 'Fp2', 'AF3', 'AF4', 'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT7', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'FT8', 'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP7', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'TP8', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO5', 'PO3', 'POz', 'PO4', 'PO6', 'PO8', 'CB1', 'O1', 'Oz', 'O2', 'CB2', 'VEO', 'HEO', 'STIM014'] original_channel\ncentral\n['Fp1', 'Fpz', 'Fp2', 'AF3', 'AF4', 'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT7', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'FT8', 'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP7', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'TP8', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO5', 'PO3', 'POz', 'PO4', 'PO6', 'PO8', 'CB1', 'O1', 'Oz', 'O2', 'CB2', 'VEO', 'HEO', 'STIM014'] original_channel\nparietal\n['Fp1', 'Fpz', 'Fp2', 'AF3', 'AF4', 'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT7', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'FT8', 'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP7', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'TP8', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO5', 'PO3', 'POz', 'PO4', 'PO6', 'PO8', 'CB1', 'O1', 'Oz', 'O2', 'CB2', 'VEO', 'HEO', 'STIM014'] original_channel\ntemporal\n(128, 951720)\n0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m x\u001b[38;5;241m=\u001b[39mchunk_raw\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 92\u001b[0m \u001b[43mchunk_raw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroot_folder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunk_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msubject_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msession_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrun_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.fif\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m<decorator-gen-223>:12\u001b[0m, in \u001b[0;36msave\u001b[0;34m(self, fname, picks, tmin, tmax, buffer_size_sec, drop_small_buffer, proj, fmt, overwrite, split_size, split_naming, verbose)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mne/io/base.py:1703\u001b[0m, in \u001b[0;36mBaseRaw.save\u001b[0;34m(self, fname, picks, tmin, tmax, buffer_size_sec, drop_small_buffer, proj, fmt, overwrite, split_size, split_naming, verbose)\u001b[0m\n\u001b[1;32m   1701\u001b[0m cfg \u001b[38;5;241m=\u001b[39m _RawFidWriterCfg(buffer_size, split_size, drop_small_buffer, fmt)\n\u001b[1;32m   1702\u001b[0m raw_fid_writer \u001b[38;5;241m=\u001b[39m _RawFidWriter(\u001b[38;5;28mself\u001b[39m, info, picks, projector, start, stop, cfg)\n\u001b[0;32m-> 1703\u001b[0m \u001b[43m_write_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_fid_writer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_naming\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mne/io/base.py:2579\u001b[0m, in \u001b[0;36m_write_raw\u001b[0;34m(raw_fid_writer, fpath, split_naming, overwrite)\u001b[0m\n\u001b[1;32m   2577\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_fpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m start_and_end_file(use_fpath) \u001b[38;5;28;01mas\u001b[39;00m fid, reserved_ctx:\n\u001b[0;32m-> 2579\u001b[0m     is_next_split \u001b[38;5;241m=\u001b[39m \u001b[43mraw_fid_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_fname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_fname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2580\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_fpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bids_special_behavior \u001b[38;5;129;01mand\u001b[39;00m is_next_split:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mne/io/base.py:2655\u001b[0m, in \u001b[0;36m_RawFidWriter.write\u001b[0;34m(self, fid, part_idx, prev_fname, next_fname)\u001b[0m\n\u001b[1;32m   2647\u001b[0m start_block(fid, FIFF\u001b[38;5;241m.\u001b[39mFIFFB_MEAS)\n\u001b[1;32m   2648\u001b[0m _write_raw_metadata(\n\u001b[1;32m   2649\u001b[0m     fid,\n\u001b[1;32m   2650\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mannotations,\n\u001b[1;32m   2654\u001b[0m )\n\u001b[0;32m-> 2655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m=\u001b[39m \u001b[43m_write_raw_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2657\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpart_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2661\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2662\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2663\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_fname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2665\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_fname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2667\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2668\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_small_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2669\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2671\u001b[0m end_block(fid, FIFF\u001b[38;5;241m.\u001b[39mFIFFB_MEAS)\n\u001b[1;32m   2672\u001b[0m is_next_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mne/io/base.py:2775\u001b[0m, in \u001b[0;36m_write_raw_data\u001b[0;34m(raw, info, picks, fid, part_idx, start, stop, buffer_size, prev_fname, split_size, next_fname, projector, drop_small_buffer, fmt)\u001b[0m\n\u001b[1;32m   2773\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   2774\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting FIF \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ... \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2775\u001b[0m \u001b[43m_write_raw_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2777\u001b[0m pos \u001b[38;5;241m=\u001b[39m fid\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   2778\u001b[0m this_buff_size_bytes \u001b[38;5;241m=\u001b[39m pos \u001b[38;5;241m-\u001b[39m pos_prev\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mne/io/base.py:2898\u001b[0m, in \u001b[0;36m_write_raw_buffer\u001b[0;34m(fid, buf, cals, fmt)\u001b[0m\n\u001b[1;32m   2893\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2894\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdouble\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported for \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwriting complex data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2895\u001b[0m         )\n\u001b[1;32m   2897\u001b[0m buf \u001b[38;5;241m=\u001b[39m buf \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mravel(cals)[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m-> 2898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_int:\n\u001b[1;32m   2899\u001b[0m     buf \u001b[38;5;241m=\u001b[39m buf\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m   2900\u001b[0m write_function(fid, FIFF\u001b[38;5;241m.\u001b[39mFIFF_DATA_BUFFER, buf)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}